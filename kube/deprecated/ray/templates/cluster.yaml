{{- $root := . -}}
apiVersion: cluster.ray.io/v1
kind: RayCluster
metadata:
  name: {{ .Release.Name }}
spec:
  maxWorkers: {{ include "ray.clusterMaxWorkers" . }}
  upscalingSpeed: 1.0
  # If a node is idle for this many minutes, it will be removed.
  idleTimeoutMinutes: 5
  # Specify the pod type for the ray head node (as configured below).
  headPodType: rayHeadNode
  headServicePorts: {{- toYaml .Values.headServicePorts | nindent 4 }}
  podTypes:
  - name: rayHeadNode
    # Minimum number of Ray workers of this Pod type.
    minWorkers: {{.Values.head.minWorkers}}
    # Maximum number of Ray workers of this Pod type. Takes precedence over minWorkers.
    maxWorkers: {{.Values.head.maxWorkers}}  
    podConfig:
      apiVersion: v1
      kind: Pod
      metadata:
        # Automatically generates a name for the pod with this prefix.
        generateName: "ray-head-"
      spec:
        restartPolicy: Never
        # This volume allocates shared memory for Ray to use for its plasma
        # object store. If you do not provide this, Ray will fall back to
        # /tmp which cause slowdowns if is not a shared memory volume.
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory        
        {{- if .Values.head.tolerations }}
        tolerations: {{- toYaml .Values.head.tolerations | nindent 8 }}
        {{- end }}
        {{- if .Values.head.nodeSelector }}
        nodeSelector: {{- toYaml .Values.head.nodeSelector | nindent 10 }}
        {{- end }}
        {{- if .Values.head.affinity }}
        affinity: {{- toYaml .Values.head.affinity | nindent 8 }}
        {{- end }}
        {{- if .Values.imagePullSecrets }}
        imagePullSecrets: {{- toYaml .Values.imagePullSecrets | nindent 8 }}
        {{- end }}
        containers:
        - name: ray-node
          imagePullPolicy: Always
          image: {{.Values.head.image}}
          # Do not change this command - it keeps the pod alive until it is
          # explicitly killed.
          command: ["/bin/bash", "-c", "--"]
          args: 
          - |
            trap : TERM INT;sleep infinity & wait;
          securityContext:
            runAsUser: 0              
          ports:
          - containerPort: 6379  # Redis port
          - containerPort: 10001  # Used by Ray Client
          - containerPort: 8265  # Used by Ray Dashboard
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          env:
          - name: TZ
            value: Asia/Tokyo
          {{- if .numGPU }}
          - name: CUDAHOME
            value: /usr/local/cuda/
          - name: CUDA_VISIBLE_DEVICES
            value: "{{.numGPU}}"
          {{- end }}            
          {{- if or (.Values.head.numCPU) (.Values.head.memory)}}
          resources:
            requests:
              {{- if .Values.head.numCPU }}
              cpu: {{ .Values.head.numCPU }}
              {{- end }}
              {{- if .Values.head.memory }}
              memory: {{ .Values.head.memory }}
              {{- end }}
            limits:
              {{- if .Values.head.numCPU }}
              cpu: {{ .Values.head.numCPU }}
              {{- end }}
              {{- if .Values.head.memory }}
              memory: {{ .Values.head.memory }}
              {{- end }}
              {{- if .Values.head.numGPU }}
              nvidia.com/gpu: {{ .Values.head.numGPU }}
              {{- end }}
          {{- end }}
{{- range $key, $val := .Values.workers }}
  - name: {{ $key }}
    minWorkers: {{ $val.minWorkers }}
    maxWorkers: {{ $val.maxWorkers }}
    {{- if $val.rayResources }}
    rayResources:
      {{- toYaml $val.rayResources | nindent 8 }}
    {{- end }}
    podConfig:
      apiVersion: v1
      kind: Pod
      metadata:
        generateName: {{ kebabcase $key }}-
      spec:
        restartPolicy: Never
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        {{- if .tolerations }}
        tolerations: {{- toYaml .tolerations | nindent 8 }}
        {{- end }}
        {{- if .nodeSelector }}
        nodeSelector: {{- toYaml .nodeSelector | nindent 10 }}
        {{- end }}
        {{- if .affinity }}
        affinity: {{- toYaml .affinity | nindent 8 }}
        {{- end }}
        {{- if $root.Values.imagePullSecrets }}
        imagePullSecrets: {{- toYaml $root.Values.imagePullSecrets | nindent 8 }}
        {{- end }}
        containers:
        - name: ray-node
          imagePullPolicy: Always
          image: {{ .image }}
          # Do not change this command - it keeps the pod alive until it is
          # explicitly killed.
          command: ["/bin/bash", "-c", "--"]
          args: 
          - | 
            trap : TERM INT; sleep infinity & wait;
          ports:
          - containerPort: 6379  # Redis port
          - containerPort: 10001  # Used by Ray Client
          - containerPort: 8265  # Used by Ray Dashboard
          - containerPort: 8000 # Used by Ray Serve
          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          env:
            - name: TZ
              value: Asia/Tokyo
            {{- if .numGPU }}
            - name: CUDAHOME
              value: /usr/local/cuda/
            - name: CUDA_VISIBLE_DEVICES
              value: "{{.numGPU}}"
            {{- end }}
          {{- if or .numCPU .numGPU }}
          resources:
            {{- if or .numCPU .memory }}            
            requests:
              {{- if .numCPU }}
              cpu: {{ .numCPU }}
              {{- end }}
              {{- if .memory }}
              memory: {{ .memory }}
              {{- end }}
            {{- end }}
            limits:
              {{- if .numCPU }}
              cpu: {{ .numCPU }}
              {{- end }}
              {{- if .memory }}
              memory: {{ .memory }}
              {{- end }}
              {{- if .numGPU }}
              nvidia.com/gpu: {{ .numGPU }}
              {{- end }}
          {{- end }}        
{{- end }}
  headStartRayCommands: {{- toYaml .Values.headStartRayCommands  | nindent 4}}
  workerStartRayCommands: {{- toYaml .Values.workerStartRayCommands  | nindent 4}}