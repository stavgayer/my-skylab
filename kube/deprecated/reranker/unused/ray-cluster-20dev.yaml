
---
apiVersion: cluster.ray.io/v1
kind: RayCluster
metadata:
  name: "{{.Chart.Name}}"  
  namespace: "{{ .Release.Namespace }}"
spec:
  # The maximum number of workers nodes to launch in addition to the head node.
  maxWorkers: 5
  # The autoscaler will scale up the cluster faster with higher upscaling speed.
  # E.g., if the task requires adding more nodes then autoscaler will gradually
  # scale up the cluster in chunks of upscaling_speed*currently_running_nodes.
  # This number should be > 0.
  upscalingSpeed: 1.0
  # If a node is idle for this many minutes, it will be removed.
  idleTimeoutMinutes: 5
  # Specify the pod type for the ray head node (as configured below).
  headPodType: head-node
  # Optionally, configure ports for the Ray head service.
  # The ports specified below are the defaults.  
  headServicePorts:
    - name: client
      port: 10001
      targetPort: 10001
    - name: dashboard
      port: 8265
      targetPort: 8265
    - name: ray-serve
      port: 8000
      targetPort: 8000  
    - name: jupyter
      port: 8888
      targetPort: 8888
    - name: tensorboard      
      port: 6006
      targetPort: 6006
  # Specify the allowed pod types for this ray cluster and the resources they provide.
  podTypes:
  - name: head-node
    # Minimum number of Ray workers of this Pod type.
    minWorkers: 0
    # Maximum number of Ray workers of this Pod type. Takes precedence over minWorkers.
    maxWorkers: 0
    podConfig:
      apiVersion: v1
      kind: Pod
      metadata:
        # Automatically generates a name for the pod with this prefix.
        generateName: "{{.Chart.Name}}-ray-head-"
      spec:
        restartPolicy: Never
        # This volume allocates shared memory for Ray to use for its plasma
        # object store. If you do not provide this, Ray will fall back to
        # /tmp which cause slowdowns if is not a shared memory volume.
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        {{- if .Values.tolerations }}
        tolerations: {{- toYaml .Values.tolerations | nindent 8 }}
        {{- end }}
        {{- if .Values.nodeSelector }}
        nodeSelector: {{- toYaml .Values.nodeSelector | nindent 10 }}
        {{- end }}
        containers:
        - name: ray-node
          imagePullPolicy: Always
          image: {{.Values.image}}
          # Do not change this command - it keeps the pod alive until it is
          # explicitly killed.
          command: ["/bin/bash", "-c", "--"]
          args:
            - |
              git config --global user.name "$GIT_USERNAME"; git config --global user.email $GIT_USEREMAIL; git clone $GIT_REPO $GIT_FOLDERNAME; \
              (gitautopush ./$GIT_FOLDERNAME --sleep 10 &); \
              (jupyter lab --allow-root --ip=0.0.0.0 &) || true; \
              trap : TERM INT;sleep infinity & wait;
          ports:
          - containerPort: 6379  # Redis port
          - containerPort: 10001  # Used by Ray Client
          - containerPort: 8265  # Used by Ray Dashboard
          - containerPort: 8888  # Used by Jupyter Lab
          - containerPort: 6006  # Used by Tensorboard
          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          env:
            - name: TZ
              value: Asia/Tokyo
            - name: GIT_REPO
              value: {{.Values.git_repo}}
            - name: GIT_USERNAME
              value: {{.Values.git_username}}              
            - name: GIT_USEREMAIL
              value: {{.Values.git_useremail}}
            - name: GIT_FOLDERNAME
              value: {{.Values.git_foldername}}
          envFrom: 
          - secretRef:
              name: aws-secrets
          {{- if .Values.head.resources }}
          resources: {{- toYaml .Values.head.resources | nindent 12 }}
          {{- end }}
  - name: worker-node
    # Minimum number of Ray workers of this Pod type.
    minWorkers: 2
    # Maximum number of Ray workers of this Pod type. Takes precedence over minWorkers.
    maxWorkers: 5
    # User-specified custom resources for use by Ray.
    # (Ray detects CPU and GPU from pod spec resource requests and limits, so no need to fill those here.)
    rayResources: {"foo": 1, "bar": 1}
    podConfig:
      apiVersion: v1
      kind: Pod
      metadata:
        # Automatically generates a name for the pod with this prefix.
        generateName: "{{.Chart.Name}}-ray-worker-"
      spec:
        restartPolicy: Never
        volumes:
        - name: dshm
          emptyDir:
            medium: Memory
        {{- if .Values.tolerations }}
        tolerations: {{- toYaml .Values.tolerations | nindent 8 }}
        {{- end }}
        {{- if .Values.nodeSelector }}
        nodeSelector: {{- toYaml .Values.nodeSelector | nindent 10 }}
        {{- end }}
        {{- if .Values.affinity }}
        affinity: {{- toYaml .Values.affinity | nindent 10 }}
        {{- end }}
        containers:
        - name: ray-node
          imagePullPolicy: Always
          image: {{.Values.image}}
          command: ["/bin/bash", "-c", "--"]
          args: ["trap : TERM INT; sleep infinity & wait;"]
          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumeMounts:
          - mountPath: /dev/shm
            name: dshm
          envFrom: 
          - secretRef:
              name: aws-secrets
          {{- if .Values.worker.resources }}
          resources: {{- toYaml .Values.worker.resources | nindent 12 }}
          {{- end }}              
  # Commands to start Ray on the head node. You don't need to change this.
  # Note dashboard-host is set to 0.0.0.0 so that Kubernetes can port forward.
  headStartRayCommands:
    - ray stop
    - ulimit -n 65536; RAY_SCHEDULER_LOADBALANCE_SPILLBACK=1 ray start --head --num-cpus=0 --no-monitor --dashboard-host 0.0.0.0; 
  # Commands to start Ray on worker nodes. You don't need to change this.
  workerStartRayCommands:
    - ray stop
    - ulimit -n 65536; RAY_SCHEDULER_LOADBALANCE_SPILLBACK=1 ray start --address=$RAY_HEAD_IP:6379